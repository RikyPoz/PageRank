{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba70193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a406f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sparse_link_matrix(filename):\n",
    "    \"\"\"\n",
    "    Constructs the Link Matrix A in Sparse Format (CSR).\n",
    "    \n",
    "    CRITICAL OPTIMIZATION:\n",
    "    This function does NOT physically apply the 'dangling node patch' (filling \n",
    "    columns with 1/N). Doing so would destroy sparsity and consume O(N^2) memory.\n",
    "    Instead, it identifies dangling nodes so we can handle them 'virtually' \n",
    "    during the calculation phase.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Path to the .dat file.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (A_sparse, N, dangling_indices)\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    out_degree = {}\n",
    "    \n",
    "    # 1) READING THE DATASET FILE\n",
    "    print(f\"Reading file: {filename}...\")\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            # 1. Read Header (N = Nodes, M = Edges)\n",
    "            header = file.readline().strip().split()\n",
    "            if not header: raise ValueError(\"File is empty or header is missing.\")\n",
    "            N = int(header[0])\n",
    "            \n",
    "            # 2. Skip URL mapping lines (we only need the graph structure)\n",
    "            # The first N lines are strings/URLs which we don't need for the math.\n",
    "            for _ in range(N): file.readline()\n",
    "            \n",
    "            # 3. Read Edges\n",
    "            # Format: \"Source_ID Target_ID\"\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    src, tgt = int(parts[0]), int(parts[1])\n",
    "                    \n",
    "                    out_degree[src] = out_degree.get(src, 0) + 1\n",
    "                    links.append((src, tgt))\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None, 0, None\n",
    "\n",
    "    # 2) SPARSE MATRIX CONSTRUCTION ---\n",
    "    # We use the COO format logic initially: separate lists for Data, Rows, Cols\n",
    "    data = []\n",
    "    rows = [] # Target Node (i)\n",
    "    cols = [] # Source Node (j)\n",
    "    \n",
    "    # Identify Dangling Nodes (Nodes with 0 out-degree)\n",
    "    # Initialize a boolean mask: assume ALL are dangling initially.\n",
    "    is_dangling = np.ones(N, dtype=bool) \n",
    "    \n",
    "    for src, tgt in links:\n",
    "        # Adjust 1-based IDs from file to 0-based Indices for Python\n",
    "        src_idx = src - 1\n",
    "        tgt_idx = tgt - 1\n",
    "        \n",
    "        # If a node appears as a source, it has outgoing links -> Not Dangling\n",
    "        is_dangling[src_idx] = False\n",
    "        \n",
    "        # Calculate transition probability: 1 / (Number of outgoing links)\n",
    "        # This makes valid columns sum to 1.\n",
    "        val = 1.0 / out_degree[src]\n",
    "        \n",
    "        rows.append(tgt_idx)\n",
    "        cols.append(src_idx)\n",
    "        data.append(val)\n",
    "        \n",
    "    # Convert to CSR format.\n",
    "    # CSR is extremely efficient for Matrix-Vector multiplication (A @ x).\n",
    "    A_sparse = sparse.csr_matrix((data, (rows, cols)), shape=(N, N))\n",
    "    \n",
    "    # Extract indices of dangling nodes for later use\n",
    "    dangling_indices = np.where(is_dangling)[0]\n",
    "    \n",
    "    print(f\"Sparse Matrix Constructed: {N} nodes.\")\n",
    "    print(f\"Valid Non-Zero Links: {len(data)}.\")\n",
    "    print(f\"Dangling Nodes Identified: {len(dangling_indices)}.\")\n",
    "    \n",
    "    return A_sparse, N, dangling_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4beb06c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank(A_sparse, N, dangling_indices, m, max_iter=200, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Computes PageRank using the Power Method with Sparse Optimizations.\n",
    "    \n",
    "    Mathematical Logic:\n",
    "    x_new = (1-m)*Ax + (1-m)*[Dangling_Correction] + m/N\n",
    "    \n",
    "    Args:\n",
    "        A_sparse: The sparse link matrix (missing dangling connections).\n",
    "        N: Total nodes.\n",
    "        dangling_indices: List of nodes that have no outgoing links.\n",
    "        m: Teleportation probability (1-m is the damping factor).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (PageRank Vector, Iterations)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Initialization\n",
    "    # Start with uniform probability distribution (1/N for everyone)\n",
    "    x = np.full(N, 1.0/N)\n",
    "    \n",
    "    # 2. Teleportation Constant\n",
    "    # This is the \"m * s\" part of the formula.\n",
    "    # Since s is [1/N, 1/N...], this term is just the scalar m/N added to every node.\n",
    "    teleport_contribution = m / N\n",
    "    \n",
    "    # Ensure dangling_indices is a numpy array for fast indexing\n",
    "    if not isinstance(dangling_indices, np.ndarray):\n",
    "        dangling_indices = np.array(dangling_indices)\n",
    "        \n",
    "    iterations = 0\n",
    "    \n",
    "    # --- POWER METHOD LOOP ---\n",
    "    for k in range(max_iter):\n",
    "        x_prev = x.copy()\n",
    "        \n",
    "        # Step A: Standard Matrix Multiplication\n",
    "        # This calculates flow only from nodes that have existing links.\n",
    "        Ax = A_sparse.dot(x_prev)\n",
    "        \n",
    "        # Step B: Implicit Dangling Node Handling\n",
    "        # Total mass from dangling nodes lost.\n",
    "        dangling_mass_sum = np.sum(x_prev[dangling_indices])\n",
    "        \n",
    "        # We redistribute this lost mass evenly to ALL nodes (1/N), \n",
    "        # applied with the damping factor (1-m).\n",
    "        dangling_correction = (1 - m) * (dangling_mass_sum / N)\n",
    "        \n",
    "        # Step C: Combine Everything\n",
    "        # 1. Flow from Links\n",
    "        # 2. Flow from Dangling Patch\n",
    "        # 3. Flow from Random Teleport\n",
    "        x = (1 - m) * Ax + dangling_correction + teleport_contribution\n",
    "        \n",
    "        # Step D: Convergence Check (L1 Norm)\n",
    "        diff = np.sum(np.abs(x - x_prev))\n",
    "        iterations = k + 1\n",
    "        \n",
    "        if diff < tol:\n",
    "            break\n",
    "            \n",
    "    return x, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e343ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "  EXECUTION: WEB WITH 4 PAGES (FIGURE 2.1) \n",
      "=============================================\n",
      "Page 1: 0.3682\n",
      "Page 3: 0.2880\n",
      "Page 4: 0.2021\n",
      "Page 2: 0.1418\n",
      "Calculation completed in 21 iterations.\n",
      "\n",
      "\n",
      "=============================================\n",
      "  EXECUTION: WEB WITH 5 PAGES (FIGURE 2.2) \n",
      "=============================================\n",
      "Page 3: 0.2850\n",
      "Page 4: 0.2850\n",
      "Page 1: 0.2000\n",
      "Page 2: 0.2000\n",
      "Page 5: 0.0300\n",
      "Calculation completed in 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Test on the required graphs.\n",
    "print(\"=============================================\")\n",
    "print(\"  EXECUTION: WEB WITH 4 PAGES (FIGURE 2.1) \")\n",
    "print(\"=============================================\")\n",
    "\n",
    "# Construction of the link matrix\n",
    "A_4pages = np.array([\n",
    "    [0.0, 0.0, 1.0, 0.5],\n",
    "    [1/3, 0.0, 0.0, 0.0],\n",
    "    [1/3, 0.5, 0.0, 0.5],\n",
    "    [1/3, 0.5, 0.0, 0.0]\n",
    "])\n",
    "\n",
    "# Calculation of PageRank\n",
    "pagerank_scores_4pages, iterations_4pages = calculate_pagerank(A_4pages, A_4pages.shape[0], False, m=0.15)\n",
    "\n",
    "# Preparation of results: list of tuples (Page ID, Score)\n",
    "page_indices_4pages = np.arange(1, A_4pages.shape[0] + 1)\n",
    "results_4pages = list(zip(page_indices_4pages, pagerank_scores_4pages.flatten()))\n",
    "results_4pages_sorted = sorted(results_4pages, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Printing the results\n",
    "for page_id, score in results_4pages_sorted:\n",
    "    print(f\"Page {page_id}: {score:.4f}\")\n",
    "    \n",
    "print(f\"Calculation completed in {iterations_4pages} iterations.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n=============================================\")\n",
    "print(\"  EXECUTION: WEB WITH 5 PAGES (FIGURE 2.2) \")\n",
    "print(\"=============================================\")\n",
    "\n",
    "#Construction of the link matrix\n",
    "A_5pages = np.array([\n",
    "    [0.0, 1.0, 0.0, 0.0, 0.0],   \n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0],  \n",
    "    [0.0, 0.0, 0.0, 1.0, 0.5],   \n",
    "    [0.0, 0.0, 1.0, 0.0, 0.5],   \n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0]    \n",
    "])\n",
    "\n",
    "# Calculation of PageRank\n",
    "pagerank_scores_5pages,iterations_5pages = calculate_pagerank(A_5pages, A_5pages.shape[0], False, m=0.15)\n",
    "\n",
    "# Preparation of results: list of tuples (Page ID, Score)\n",
    "page_indices_5pages = np.arange(1, A_5pages.shape[0] + 1)\n",
    "results_5pages = list(zip(page_indices_5pages, pagerank_scores_5pages.flatten()))\n",
    "results_5pages_sorted = sorted(results_5pages, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Printing the results\n",
    "for page_id, score in results_5pages_sorted:\n",
    "    print(f\"Page {page_id}: {score:.4f}\")\n",
    "    \n",
    "print(f\"Calculation completed in {iterations_5pages} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f088cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: hollins.dat...\n",
      "Sparse Matrix Constructed: 6012 nodes.\n",
      "Valid Non-Zero Links: 23875.\n",
      "Dangling Nodes Identified: 3189.\n",
      "Calculation converged in 71 iterations.\n",
      "\n",
      "--- TOP 10 RANKING ---\n",
      "Rank  | Page ID    | Score          \n",
      "-----------------------------------\n",
      "1     | 2          | 0.019879\n",
      "2     | 37         | 0.009288\n",
      "3     | 38         | 0.008610\n",
      "4     | 61         | 0.008065\n",
      "5     | 52         | 0.008027\n",
      "6     | 43         | 0.007165\n",
      "7     | 425        | 0.006583\n",
      "8     | 27         | 0.005989\n",
      "9     | 28         | 0.005572\n",
      "10    | 4023       | 0.004452\n",
      "\n",
      "--- VERIFICATION ---\n",
      "Total Probability Sum: 1.000000 (Should be 1.0)\n",
      "Lowest PageRank Score:   0.00005806\n",
      "Theoretical Minimum (m/N): 0.00002495\n"
     ]
    }
   ],
   "source": [
    "filename = 'hollins.dat'\n",
    "m = 0.15      # Standard Google damping factor\n",
    "top_k = 10    # Number of top results to display\n",
    "\n",
    "# 1. Build the Matrix\n",
    "A_sparse, N, dangling_nodes = build_sparse_link_matrix(filename)\n",
    "\n",
    "# 2. Calculate PageRank\n",
    "if A_sparse is not None:\n",
    "    pagerank_scores, iters = calculate_pagerank(A_sparse, N, dangling_nodes, m=m)\n",
    "\n",
    "    print(f\"Calculation converged in {iters} iterations.\")\n",
    "\n",
    "    # 3. Display Results\n",
    "    # Create pairs of (Page ID, Score)\n",
    "    page_ids = np.arange(1, N + 1)\n",
    "    results = list(zip(page_ids, pagerank_scores))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    results_sorted = sorted(results, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    print(f\"\\n--- TOP {top_k} RANKING ---\")\n",
    "    print(f\"{'Rank':<5} | {'Page ID':<10} | {'Score':<15}\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for rank, (page_id, score) in enumerate(results_sorted[:top_k], 1):\n",
    "        print(f\"{rank:<5} | {page_id:<10} | {score:.6f}\")\n",
    "\n",
    "    # 4. Mathematical Verification\n",
    "    total_prob = np.sum(pagerank_scores)\n",
    "    print(\"\\n--- VERIFICATION ---\")\n",
    "    print(f\"Total Probability Sum: {total_prob:.6f} (Should be 1.0)\")\n",
    "    \n",
    "    # Minimum Score Check\n",
    "    min_score = results_sorted[-1][1]\n",
    "    expected_min = m / N\n",
    "    print(f\"Lowest PageRank Score:   {min_score:.8f}\")\n",
    "    print(f\"Theoretical Minimum (m/N): {expected_min:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1815187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "                 EXERCISE 1\n",
      "=============================================\n",
      "\n",
      "--- 1. Original Situation (4 Pages) ---\n",
      "Original Ranking:\n",
      "Page 1: 0.3871\n",
      "Page 3: 0.2903\n",
      "Page 4: 0.1935\n",
      "Page 2: 0.1290\n",
      "Calculation completed in 27 iterations.\n",
      "\n",
      "--- 2. Modified Situation (5 Pages) ---\n",
      "Modification: Added Page 5. Links: 3->5 and 5->3.\n",
      "Page 3: 0.3673\n",
      "Page 1: 0.2449\n",
      "Page 5: 0.1837\n",
      "Page 4: 0.1224\n",
      "Page 2: 0.0816\n",
      "Calculation completed in 49 iterations.\n",
      "\n",
      "=============================================\n",
      "               RESULTS CHECK\n",
      "=============================================\n",
      "BEFORE: Page 1 (0.3871) vs Page 3 (0.2903)\n",
      "AFTER:  Page 1 (0.2449) vs Page 3 (0.3673)\n",
      "\n",
      "ANSWER: YES. The strategy worked.\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================================\")\n",
    "print(\"                 EXERCISE 1\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Original Situation (4 Pages)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- 1. Original Situation (4 Pages) ---\")\n",
    "\n",
    "A_orig_dense = np.array([\n",
    "    [0.0, 0.0, 1.0, 0.5],  \n",
    "    [1/3, 0.0, 0.0, 0.0],  \n",
    "    [1/3, 0.5, 0.0, 0.5],  \n",
    "    [1/3, 0.5, 0.0, 0.0]   \n",
    "])\n",
    "\n",
    "# Convert to sparse and calculate\n",
    "# We pass [] as dangling_indices because this matrix is strictly column-stochastic (cols sum to 1)\n",
    "A_orig_sparse = sparse.csr_matrix(A_orig_dense)\n",
    "scores_orig, iters_orig = calculate_pagerank(A_orig_sparse,4,False,m=0.0)\n",
    "\n",
    "# Print Original Results\n",
    "indices_orig = np.arange(1, 5)\n",
    "results_orig = list(zip(indices_orig, scores_orig))\n",
    "results_orig_sorted = sorted(results_orig, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Original Ranking:\")\n",
    "for page_id, score in results_orig_sorted:\n",
    "    print(f\"Page {page_id}: {score:.4f}\")\n",
    "\n",
    "print(f\"Calculation completed in {iters_orig} iterations.\")\n",
    "\n",
    "# FIX: Access as 1D array\n",
    "p1_score_orig = scores_orig[0]\n",
    "p3_score_orig = scores_orig[2]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Modified Situation (5 Pages)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- 2. Modified Situation (5 Pages) ---\")\n",
    "print(\"Modification: Added Page 5. Links: 3->5 and 5->3.\")\n",
    "\n",
    "# Modified A Matrix (5x5)\n",
    "A_mod_dense = np.array([\n",
    "    [0.0, 0.0, 0.5, 0.5, 0.0],  \n",
    "    [1/3, 0.0, 0.0, 0.0, 0.0],  \n",
    "    [1/3, 0.5, 0.0, 0.5, 1.0],  \n",
    "    [1/3, 0.5, 0.0, 0.0, 0.0],  \n",
    "    [0.0, 0.0, 0.5, 0.0, 0.0]   \n",
    "])\n",
    "\n",
    "A_mod_sparse = sparse.csr_matrix(A_mod_dense)\n",
    "scores_mod, iters_mod = calculate_pagerank(A_mod_sparse,5,False,m=0.0)\n",
    "\n",
    "# Print Modified Results\n",
    "indices_mod = np.arange(1, 6)\n",
    "results_mod = list(zip(indices_mod, scores_mod))\n",
    "results_mod_sorted = sorted(results_mod, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "for page_id, score in results_mod_sorted:\n",
    "    print(f\"Page {page_id}: {score:.4f}\")\n",
    "\n",
    "print(f\"Calculation completed in {iters_mod} iterations.\")\n",
    "\n",
    "# Access as 1D array\n",
    "p1_score_mod = scores_mod[0]\n",
    "p3_score_mod = scores_mod[2]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FINAL CHECK\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n=============================================\")\n",
    "print(\"               RESULTS CHECK\")\n",
    "print(\"=============================================\")\n",
    "print(f\"BEFORE: Page 1 ({p1_score_orig:.4f}) vs Page 3 ({p3_score_orig:.4f})\")\n",
    "print(f\"AFTER:  Page 1 ({p1_score_mod:.4f}) vs Page 3 ({p3_score_mod:.4f})\")\n",
    "\n",
    "if p3_score_mod > p1_score_mod:\n",
    "    print(\"\\nANSWER: YES. The strategy worked.\")\n",
    "else:\n",
    "    print(\"\\nANSWER: NO.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b8a7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      " EXERCISE 11: PAGERANK CALCULATION ON MODIFIED NETWORK\n",
      " (Eigenvector of M with m=0.15)\n",
      "=============================================\n",
      "\n",
      "Calculation converged in 33 iterations.\n",
      "Final Ranking:\n",
      "Page 3: 0.3489\n",
      "Page 1: 0.2371\n",
      "Page 5: 0.1783\n",
      "Page 4: 0.1385\n",
      "Page 2: 0.0972\n",
      "\n",
      "Key Comparison:\n",
      "Page 3 (0.3489) > Page 1 (0.2371)?\n",
      "YES\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================================\")\n",
    "print(\" EXERCISE 11: PAGERANK CALCULATION ON MODIFIED NETWORK\")\n",
    "print(\" (Eigenvector of M with m=0.15)\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "A_ex11_dense = np.array([\n",
    "    [0.0, 0.0, 0.5, 0.5, 0.0],  \n",
    "    [1/3, 0.0, 0.0, 0.0, 0.0],  \n",
    "    [1/3, 0.5, 0.0, 0.5, 1.0],  \n",
    "    [1/3, 0.5, 0.0, 0.0, 0.0],  \n",
    "    [0.0, 0.0, 0.5, 0.0, 0.0]   \n",
    "])\n",
    "\n",
    "A_ex11_sparse = sparse.csr_matrix(A_ex11_dense)\n",
    "scores_ex11, iters_ex11 = calculate_pagerank(A_ex11_sparse, 5, False, m=0.15)\n",
    "\n",
    "print(f\"\\nCalculation converged in {iters_ex11} iterations.\")\n",
    "print(\"Final Ranking:\")\n",
    "\n",
    "indices_ex11 = np.arange(1, 6)\n",
    "results_ex11 = list(zip(indices_ex11, scores_ex11))\n",
    "results_ex11_sorted = sorted(results_ex11, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "for page_id, score in results_ex11_sorted:\n",
    "    print(f\"Page {page_id}: {score:.4f}\")\n",
    "\n",
    "p1 = scores_ex11[0]\n",
    "p3 = scores_ex11[2]\n",
    "print(f\"\\nKey Comparison:\")\n",
    "print(f\"Page 3 ({p3:.4f}) > Page 1 ({p1:.4f})?\")\n",
    "print(\"YES\" if p3 > p1 else \"NO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
