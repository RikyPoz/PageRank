{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1524c83d-e461-4a1e-a591-00d4424f239a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4fd4a-4c97-4c2e-983e-a7fb809fbc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_link_matrix_A(filename):\n",
    "    \"\"\"\n",
    "    Builds the Link Matrix A from a .dat dataset file.\n",
    "    \n",
    "    This function handles reading sparse data (edges) and creating \n",
    "    the dense or sparse matrix needed for computation. It also manages dangling nodes.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The path to the .dat file.                            \n",
    "    Returns:\n",
    "        tuple: (A, N)\n",
    "            - A (numpy.ndarray): The constructed N x N matrix.\n",
    "            - N (int): The total number of nodes (pages).\n",
    "    \"\"\"\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    # =========================================================================\n",
    "    # --- PART 1: FILE READING AND PRE-PROCESSING ---\n",
    "    # =========================================================================\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            \n",
    "            # 1. Read Header\n",
    "            # The file starts with \"N M\" (Nodes, Edges). We need N to size the matrix.\n",
    "            header_line = file.readline().strip().split()\n",
    "            if not header_line or len(header_line) < 2:\n",
    "                raise ValueError(\"The file must start with a valid 'N M' line.\")\n",
    "            \n",
    "            num_nodes = int(header_line[0])\n",
    "            N = num_nodes  \n",
    "            \n",
    "            # 2. Skip URL mapping lines\n",
    "            # The first N lines after the header are URLs (strings). \n",
    "            # For the math, we only need numeric IDs, so we skip them.\n",
    "            print(f\"Skipping the first {num_nodes} URL mapping lines...\")\n",
    "            for _ in range(num_nodes):\n",
    "                file.readline()\n",
    "            \n",
    "            # 3. Process the edges (Links)\n",
    "            # Dictionary to count how many links exit each page (Out-Degree, n_j).\n",
    "            # Used to calculate the transition probability (1 / n_j).\n",
    "            out_degree = {} \n",
    "            valid_link_count = 0\n",
    "            \n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                # We expect lines made of \"Source_ID Target_ID\"\n",
    "                if len(parts) == 2:\n",
    "                    try:\n",
    "                        source_id = int(parts[0])\n",
    "                        target_id = int(parts[1])\n",
    "                        \n",
    "                        # Count the out-degree for the source node\n",
    "                        out_degree[source_id] = out_degree.get(source_id, 0) + 1\n",
    "                        \n",
    "                        # Store the edge for later processing\n",
    "                        links.append((source_id, target_id))\n",
    "                        valid_link_count += 1\n",
    "                    except ValueError:\n",
    "                        # Robust handling for malformed lines\n",
    "                        continue\n",
    "                        \n",
    "            print(f\"Read {valid_link_count} valid edges.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the file: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "    if N == 0:\n",
    "        return None, 0\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- PART 2: BASIC CONSTRUCTION OF A (The Link Matrix) ---\n",
    "    # =========================================================================\n",
    "    \n",
    "    A = np.zeros((N, N))\n",
    "\n",
    "    # Populate the matrix using the read edges.\n",
    "    # Rule: A[i, j] = 1 / n_j  (where j=source, i=destination)\n",
    "    for source_id, target_id in links:\n",
    "        # Convert from ID (base-1, from file) to Index (base-0, for Python)\n",
    "        source_idx = source_id - 1  # Column j\n",
    "        target_idx = target_id - 1  # Row i\n",
    "        \n",
    "        # Recover n_j (how many links exit the source node)\n",
    "        n_j = out_degree.get(source_id, 0)\n",
    "        \n",
    "        # Safety check on indices and division by zero\n",
    "        if 0 <= source_idx < N and 0 <= target_idx < N and n_j > 0:\n",
    "            A[target_idx, source_idx] = 1.0 / n_j\n",
    "\n",
    "    # At this point, if a node has no outgoing links (so, it is dangling node),\n",
    "    # its column is entirely ZERO. (Substochastic Matrix).\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- PART 3: APPLYING THE PATCH ---\n",
    "    # =========================================================================\n",
    "    dangling_count = 0\n",
    "    patch_value = 1.0 / N\n",
    "    \n",
    "    # Scan all columns (source nodes)\n",
    "    for j in range(N):\n",
    "        # Check if it is a Dangling Node (out_degree = 0)\n",
    "        if out_degree.get(j + 1, 0) == 0:\n",
    "            # Replace the zero column with 1/N.\n",
    "            A[:, j] = patch_value\n",
    "            dangling_count += 1\n",
    "            \n",
    "    print(f\"Patch applied to {dangling_count} dangling nodes out of {N} total.\")\n",
    "    print(\"The resulting matrix is now perfectly column-stochastic (column sums = 1).\")\n",
    "    \n",
    "    print(f\"Link matrix {N}x{N} ready for calculation.\")\n",
    "    return A, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e013673-63a7-45fe-9d2b-d98dfcceb5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_pagerank(L_matrix, m, max_iter=200, tolerance=1e-7):\n",
    "    \"\"\"\n",
    "    Compute the PageRank using the Power Method.\n",
    "    \n",
    "    This function implements the iterative formula:\n",
    "    x(k+1) = (1 - m) * A * x(k) + m * s\n",
    "    \n",
    "    Args:\n",
    "        L_matrix (numpy.ndarray): The Link Matrix A (N x N).\n",
    "        m (float): Teleportation probability, (1-m) is the Damping Factor.\n",
    "        max_iter (int): Maximum number of iterations to avoid infinite loops.\n",
    "        tolerance (float): Convergence threshold. If the difference between two iterations is less than this, we stop.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (PageRank_eigenvector, number_of_iterations)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Determine the size of the Web (N)\n",
    "    n = L_matrix.shape[0]\n",
    "    \n",
    "    # 2. Create the Teleportation vector (s)\n",
    "    # It is a column vector where each element is 1/N.\n",
    "    s = np.full((n, 1), 1/n)\n",
    "    \n",
    "    # 3. Initialization of the PageRank vector (x)\n",
    "    # At the beginning, we assume all pages have the same importance (1/N).\n",
    "    x = np.full((n, 1), 1/n)\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    # --- START POWER METHOD ---\n",
    "    for k in range(max_iter):\n",
    "        \n",
    "        x_prev = x.copy()\n",
    "        \n",
    "        # Note: L_matrix is sparse (has many zeros), so this operation is fast.\n",
    "        Ax = L_matrix @ x_prev\n",
    "        \n",
    "        # This step simulates the multiplication by the matrix M without having to build it.\n",
    "        x = (1 - m) * Ax + m * s\n",
    "        \n",
    "        diff = np.sum(np.abs(x - x_prev))\n",
    "        \n",
    "        if diff < tolerance:\n",
    "            # Exit the for loop early if we have convergence\n",
    "            break\n",
    "            \n",
    "    # --- END POWER METHOD ---\n",
    "    \n",
    "    return x, k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889423fc-afb8-4dab-ae31-0d112871617a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "  EXECUTION: WEB WITH 4 PAGES (FIGURE 2.1) \n",
      "=============================================\n",
      "Pagina 1: 0.3682\n",
      "Pagina 3: 0.2880\n",
      "Pagina 4: 0.2021\n",
      "Pagina 2: 0.1418\n",
      "Calculation completed in 21 iterations.\n",
      "\n",
      "\n",
      "=============================================\n",
      "  EXECUTION: WEB WITH 5 PAGES (FIGURE 2.2) \n",
      "=============================================\n",
      "Page 3: 0.2850\n",
      "Page 4: 0.2850\n",
      "Page 1: 0.2000\n",
      "Page 2: 0.2000\n",
      "Page 5: 0.0300\n",
      "Calculation completed in 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================================\")\n",
    "print(\"  EXECUTION: WEB WITH 4 PAGES (FIGURE 2.1) \")\n",
    "print(\"=============================================\")\n",
    "\n",
    "# Construction of the link matrix\n",
    "A_4pages = np.array([\n",
    "    [0.0, 0.0, 1.0, 0.5],\n",
    "    [1/3, 0.0, 0.0, 0.0],\n",
    "    [1/3, 0.5, 0.0, 0.5],\n",
    "    [1/3, 0.5, 0.0, 0.0]\n",
    "])\n",
    "\n",
    "# Calculation of PageRank\n",
    "pagerank_scores_4pages,iterations_4pages = calculate_pagerank(A_4pages, m=0.15)\n",
    "\n",
    "# Preparation of results: list of tuples (Page ID, Score)\n",
    "page_indices_4pages = np.arange(1, A_4pages.shape[0] + 1)\n",
    "results_4pages = list(zip(page_indices_4pages, pagerank_scores_4pages.flatten()))\n",
    "results_4pages_sorted = sorted(results_4pages, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Printing the results\n",
    "for page_id, score in results_4pages_sorted:\n",
    "    print(f\"Pagina {page_id}: {score:.4f}\")\n",
    "    \n",
    "print(f\"Calculation completed in {iterations_4pages} iterations.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n=============================================\")\n",
    "print(\"  EXECUTION: WEB WITH 5 PAGES (FIGURE 2.2) \")\n",
    "print(\"=============================================\")\n",
    "\n",
    "#Construction of the link matrix\n",
    "A_5pages = np.array([\n",
    "    [0.0, 1.0, 0.0, 0.0, 0.0],   \n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0],  \n",
    "    [0.0, 0.0, 0.0, 1.0, 0.5],   \n",
    "    [0.0, 0.0, 1.0, 0.0, 0.5],   \n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0]    \n",
    "])\n",
    "\n",
    "# Calculation of PageRank\n",
    "pagerank_scores_5pages,iterations_5pages = calculate_pagerank(A_5pages, m=0.15)\n",
    "\n",
    "# Preparation of results: list of tuples (Page ID, Score)\n",
    "page_indices_5pages = np.arange(1, A_5pages.shape[0] + 1)\n",
    "results_5pages = list(zip(page_indices_5pages, pagerank_scores_5pages.flatten()))\n",
    "results_5pages_sorted = sorted(results_5pages, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Printing the results\n",
    "for page_id, score in results_5pages_sorted:\n",
    "    print(f\"Page {page_id}: {score:.4f}\")\n",
    "    \n",
    "print(f\"Calculation completed in {iterations_5pages} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7f432bd-01b8-4e3d-a7a9-035419119147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the first 6012 URL mapping lines...\n",
      "Read 23875 valid edges.\n",
      "Patch applied to 3189 dangling nodes out of 6012 total.\n",
      "The resulting matrix is now perfectly column-stochastic (column sums = 1).\n",
      "Link matrix 6012x6012 ready for calculation.\n",
      "Calculation completed in 71 iterations.\n",
      "\n",
      "--- TOP 10 Pages Ranking (out of 6012 total) ---\n",
      "Rank 1: Page ID 2 (Score: 0.019879)\n",
      "Rank 2: Page ID 37 (Score: 0.009288)\n",
      "Rank 3: Page ID 38 (Score: 0.008610)\n",
      "Rank 4: Page ID 61 (Score: 0.008065)\n",
      "Rank 5: Page ID 52 (Score: 0.008027)\n",
      "Rank 6: Page ID 43 (Score: 0.007165)\n",
      "Rank 7: Page ID 425 (Score: 0.006583)\n",
      "Rank 8: Page ID 27 (Score: 0.005989)\n",
      "Rank 9: Page ID 28 (Score: 0.005572)\n",
      "Rank 10: Page ID 4023 (Score: 0.004452)\n",
      "-------------------------------------------------------\n",
      "Minimum score (last page): 0.000058\n",
      "Theoretical minimum score (0.15/N): 0.000025\n"
     ]
    }
   ],
   "source": [
    "# Calculation of PageRank on the hollins.dat dataset\n",
    "filename = 'hollins.dat'\n",
    "m=0.15 \n",
    "top_k=10\n",
    "\n",
    "# 1. CONSTRUCTION OF THE LINK MATRIX A \n",
    "A_matrix, N_nodes = build_link_matrix_A(filename)\n",
    "\n",
    "# 2. CALCULATION OF PAGERANK\n",
    "pagerank_scores, iterations = calculate_pagerank(A_matrix, m=m)\n",
    "print(f\"Calculation completed in {iterations} iterations.\")\n",
    "\n",
    "# 3. PREPARATION AND PRINTING OF RESULTS\n",
    "page_ids = np.arange(1, N_nodes + 1)\n",
    "results = list(zip(page_ids, pagerank_scores.flatten()))\n",
    "results_sorted = sorted(results, key=operator.itemgetter(1), reverse=True)\n",
    "print(f\"\\n--- TOP {top_k} Pages Ranking (out of {N_nodes} total) ---\")\n",
    "\n",
    "# Print the top K results\n",
    "for rank, (page_id, score) in enumerate(results_sorted[:top_k], 1):\n",
    "    print(f\"Rank {rank}: Page ID {page_id} (Score: {score:.6f})\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# For a good evaluation, we print the importance of the least important node, which should approach the theoretical minimum score (m/N).\n",
    "min_score = results_sorted[-1][1]\n",
    "expected_min = m / N_nodes\n",
    "print(f\"Minimum score (last page): {min_score:.6f}\")\n",
    "print(f\"Theoretical minimum score ({m}/N): {expected_min:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
