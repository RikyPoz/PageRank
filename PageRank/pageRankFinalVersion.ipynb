{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "906bec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de8212f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sparse_link_matrix(filename):\n",
    "    \"\"\"\n",
    "    Constructs the Link Matrix A in Sparse Format (CSR).\n",
    "    \n",
    "    CRITICAL OPTIMIZATION:\n",
    "    This function does NOT physically apply the 'dangling node patch' (filling \n",
    "    columns with 1/N). Doing so would destroy sparsity and consume O(N^2) memory.\n",
    "    Instead, it identifies dangling nodes so we can handle them 'virtually' \n",
    "    during the calculation phase.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Path to the .dat file.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (A_sparse, N, dangling_indices)\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    out_degree = {}\n",
    "    \n",
    "    print(f\"Reading file: {filename}...\")\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            # 1. Read Header (N = Nodes, M = Edges)\n",
    "            header = file.readline().strip().split()\n",
    "            if not header: raise ValueError(\"File is empty or header is missing.\")\n",
    "            N = int(header[0])\n",
    "            \n",
    "            # 2. Skip URL mapping lines (we only need the graph structure)\n",
    "            # The first N lines are strings/URLs which we don't need for the math.\n",
    "            for _ in range(N): file.readline()\n",
    "            \n",
    "            # 3. Read Edges\n",
    "            # Format: \"Source_ID Target_ID\"\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    src, tgt = int(parts[0]), int(parts[1])\n",
    "                    \n",
    "                    out_degree[src] = out_degree.get(src, 0) + 1\n",
    "                    links.append((src, tgt))\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None, 0, None\n",
    "\n",
    "    # --- SPARSE MATRIX CONSTRUCTION ---\n",
    "    # We use the COO (Coordinate) format logic initially: separate lists for Data, Rows, Cols\n",
    "    data = []\n",
    "    rows = [] # Target Node (i)\n",
    "    cols = [] # Source Node (j)\n",
    "    \n",
    "    # Identify Dangling Nodes (Nodes with 0 out-degree)\n",
    "    # Initialize a boolean mask: assume ALL are dangling initially.\n",
    "    is_dangling = np.ones(N, dtype=bool) \n",
    "    \n",
    "    for src, tgt in links:\n",
    "        # Adjust 1-based IDs from file to 0-based Indices for Python\n",
    "        src_idx = src - 1\n",
    "        tgt_idx = tgt - 1\n",
    "        \n",
    "        # If a node appears as a source, it has outgoing links -> Not Dangling\n",
    "        is_dangling[src_idx] = False\n",
    "        \n",
    "        # Calculate transition probability: 1 / (Number of outgoing links)\n",
    "        # This makes valid columns sum to 1.\n",
    "        val = 1.0 / out_degree[src]\n",
    "        \n",
    "        rows.append(tgt_idx)\n",
    "        cols.append(src_idx)\n",
    "        data.append(val)\n",
    "        \n",
    "    # Convert to CSR (Compressed Sparse Row) format.\n",
    "    # CSR is extremely efficient for Matrix-Vector multiplication (A @ x).\n",
    "    A_sparse = sparse.csr_matrix((data, (rows, cols)), shape=(N, N))\n",
    "    \n",
    "    # Extract indices of dangling nodes for later use\n",
    "    dangling_indices = np.where(is_dangling)[0]\n",
    "    \n",
    "    print(f\"Sparse Matrix Constructed: {N} nodes.\")\n",
    "    print(f\"Valid Non-Zero Links: {len(data)}.\")\n",
    "    print(f\"Dangling Nodes Identified: {len(dangling_indices)}.\")\n",
    "    \n",
    "    return A_sparse, N, dangling_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8503af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank_sparse(A_sparse, N, dangling_indices, m=0.15, max_iter=200, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Computes PageRank using the Power Method with Sparse Optimizations.\n",
    "    \n",
    "    Mathematical Logic:\n",
    "    x_new = (1-m)*Ax + (1-m)*[Dangling_Correction] + m/N\n",
    "    \n",
    "    Args:\n",
    "        A_sparse: The sparse link matrix (missing dangling connections).\n",
    "        N: Total nodes.\n",
    "        dangling_indices: List of nodes that have no outgoing links.\n",
    "        m: Teleportation probability (1-m is the damping factor).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (PageRank Vector, Iterations)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Initialization\n",
    "    # Start with uniform probability distribution (1/N for everyone)\n",
    "    x = np.full(N, 1.0/N)\n",
    "    \n",
    "    # 2. Teleportation Constant\n",
    "    # This is the \"m * s\" part of the formula.\n",
    "    # Since s is [1/N, 1/N...], this term is just the scalar m/N added to every node.\n",
    "    teleport_contribution = m / N\n",
    "    \n",
    "    # Ensure dangling_indices is a numpy array for fast indexing\n",
    "    if not isinstance(dangling_indices, np.ndarray):\n",
    "        dangling_indices = np.array(dangling_indices)\n",
    "        \n",
    "    iterations = 0\n",
    "    \n",
    "    # --- POWER METHOD LOOP ---\n",
    "    for k in range(max_iter):\n",
    "        x_prev = x.copy()\n",
    "        \n",
    "        # Step A: Standard Matrix Multiplication\n",
    "        # This calculates flow only from nodes that have existing links.\n",
    "        # Since A_sparse is sparse, this is O(Links) complexity, not O(N^2).\n",
    "        Ax = A_sparse.dot(x_prev)\n",
    "        \n",
    "        # Step B: Implicit Dangling Node Handling\n",
    "        # Since dangling columns are 0 in A_sparse, we \"lost\" the probability mass \n",
    "        # that was sitting on those nodes. We calculate how much was lost.\n",
    "        dangling_mass_sum = np.sum(x_prev[dangling_indices])\n",
    "        \n",
    "        # We redistribute this lost mass evenly to ALL nodes (1/N), \n",
    "        # applied with the damping factor (1-m).\n",
    "        dangling_correction = (1 - m) * (dangling_mass_sum / N)\n",
    "        \n",
    "        # Step C: Combine Everything\n",
    "        # 1. Flow from Links (damped)\n",
    "        # 2. Flow from Dangling Patch (damped)\n",
    "        # 3. Flow from Random Teleport (m)\n",
    "        x = (1 - m) * Ax + dangling_correction + teleport_contribution\n",
    "        \n",
    "        # Step D: Convergence Check (L1 Norm)\n",
    "        diff = np.sum(np.abs(x - x_prev))\n",
    "        iterations = k + 1\n",
    "        \n",
    "        if diff < tol:\n",
    "            break\n",
    "            \n",
    "    return x, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab45a677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: hollins.dat...\n",
      "Sparse Matrix Constructed: 6012 nodes.\n",
      "Valid Non-Zero Links: 23875.\n",
      "Dangling Nodes Identified: 3189.\n",
      "Calculation converged in 71 iterations.\n",
      "\n",
      "--- TOP 10 RANKING ---\n",
      "Rank  | Page ID    | Score          \n",
      "-----------------------------------\n",
      "1     | 2          | 0.019879\n",
      "2     | 37         | 0.009288\n",
      "3     | 38         | 0.008610\n",
      "4     | 61         | 0.008065\n",
      "5     | 52         | 0.008027\n",
      "6     | 43         | 0.007165\n",
      "7     | 425        | 0.006583\n",
      "8     | 27         | 0.005989\n",
      "9     | 28         | 0.005572\n",
      "10    | 4023       | 0.004452\n",
      "\n",
      "--- VERIFICATION ---\n",
      "Total Probability Sum: 1.000000 (Should be 1.0)\n",
      "Lowest PageRank Score:   0.00005806\n",
      "Theoretical Minimum (m/N): 0.00002495\n"
     ]
    }
   ],
   "source": [
    "filename = 'hollins.dat'\n",
    "m = 0.15      # Standard Google damping factor\n",
    "top_k = 10    # Number of top results to display\n",
    "\n",
    "# 1. Build the Matrix\n",
    "A_sparse, N, dangling_nodes = build_sparse_link_matrix(filename)\n",
    "\n",
    "# 2. Calculate PageRank\n",
    "if A_sparse is not None:\n",
    "    pagerank_scores, iters = calculate_pagerank_sparse(A_sparse, N, dangling_nodes, m=m)\n",
    "\n",
    "    print(f\"Calculation converged in {iters} iterations.\")\n",
    "\n",
    "    # 3. Display Results\n",
    "    # Create pairs of (Page ID, Score)\n",
    "    page_ids = np.arange(1, N + 1)\n",
    "    results = list(zip(page_ids, pagerank_scores))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    results_sorted = sorted(results, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    print(f\"\\n--- TOP {top_k} RANKING ---\")\n",
    "    print(f\"{'Rank':<5} | {'Page ID':<10} | {'Score':<15}\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for rank, (page_id, score) in enumerate(results_sorted[:top_k], 1):\n",
    "        print(f\"{rank:<5} | {page_id:<10} | {score:.6f}\")\n",
    "\n",
    "    # 4. Mathematical Verification\n",
    "    total_prob = np.sum(pagerank_scores)\n",
    "    print(\"\\n--- VERIFICATION ---\")\n",
    "    print(f\"Total Probability Sum: {total_prob:.6f} (Should be 1.0)\")\n",
    "    \n",
    "    # Minimum Score Check\n",
    "    min_score = results_sorted[-1][1]\n",
    "    expected_min = m / N\n",
    "    print(f\"Lowest PageRank Score:   {min_score:.8f}\")\n",
    "    print(f\"Theoretical Minimum (m/N): {expected_min:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "753af808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "  EXECUTION: WEB WITH 4 PAGES (FIGURE 2.1) \n",
      "=============================================\n",
      "Pagina 1: 0.3682\n",
      "Pagina 3: 0.2880\n",
      "Pagina 4: 0.2021\n",
      "Pagina 2: 0.1418\n",
      "Calculation completed in 21 iterations.\n",
      "\n",
      "\n",
      "=============================================\n",
      "  EXECUTION: WEB WITH 5 PAGES (FIGURE 2.2) \n",
      "=============================================\n",
      "Page 3: 0.2850\n",
      "Page 4: 0.2850\n",
      "Page 1: 0.2000\n",
      "Page 2: 0.2000\n",
      "Page 5: 0.0300\n",
      "Calculation completed in 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Test on the required graphs.\n",
    "print(\"=============================================\")\n",
    "print(\"  EXECUTION: WEB WITH 4 PAGES (FIGURE 2.1) \")\n",
    "print(\"=============================================\")\n",
    "\n",
    "# Construction of the link matrix\n",
    "A_4pages = np.array([\n",
    "    [0.0, 0.0, 1.0, 0.5],\n",
    "    [1/3, 0.0, 0.0, 0.0],\n",
    "    [1/3, 0.5, 0.0, 0.5],\n",
    "    [1/3, 0.5, 0.0, 0.0]\n",
    "])\n",
    "\n",
    "# Calculation of PageRank\n",
    "pagerank_scores_4pages, iterations_4pages = calculate_pagerank_sparse(A_4pages, A_4pages.shape[0], False, m=0.15)\n",
    "\n",
    "# Preparation of results: list of tuples (Page ID, Score)\n",
    "page_indices_4pages = np.arange(1, A_4pages.shape[0] + 1)\n",
    "results_4pages = list(zip(page_indices_4pages, pagerank_scores_4pages.flatten()))\n",
    "results_4pages_sorted = sorted(results_4pages, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Printing the results\n",
    "for page_id, score in results_4pages_sorted:\n",
    "    print(f\"Pagina {page_id}: {score:.4f}\")\n",
    "    \n",
    "print(f\"Calculation completed in {iterations_4pages} iterations.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n=============================================\")\n",
    "print(\"  EXECUTION: WEB WITH 5 PAGES (FIGURE 2.2) \")\n",
    "print(\"=============================================\")\n",
    "\n",
    "#Construction of the link matrix\n",
    "A_5pages = np.array([\n",
    "    [0.0, 1.0, 0.0, 0.0, 0.0],   \n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0],  \n",
    "    [0.0, 0.0, 0.0, 1.0, 0.5],   \n",
    "    [0.0, 0.0, 1.0, 0.0, 0.5],   \n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0]    \n",
    "])\n",
    "\n",
    "# Calculation of PageRank\n",
    "pagerank_scores_5pages,iterations_5pages = calculate_pagerank_sparse(A_5pages, A_5pages.shape[0], False, m=0.15)\n",
    "\n",
    "# Preparation of results: list of tuples (Page ID, Score)\n",
    "page_indices_5pages = np.arange(1, A_5pages.shape[0] + 1)\n",
    "results_5pages = list(zip(page_indices_5pages, pagerank_scores_5pages.flatten()))\n",
    "results_5pages_sorted = sorted(results_5pages, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# Printing the results\n",
    "for page_id, score in results_5pages_sorted:\n",
    "    print(f\"Page {page_id}: {score:.4f}\")\n",
    "    \n",
    "print(f\"Calculation completed in {iterations_5pages} iterations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
